---
title: "Data preparation"
format: html
editor: visual
editor_options: 
  chunk_output_type: console
---

## Information

This script is a general preparation for the data. It retrieves the data and does some basic processing. In addition, it distinguished the data from the different surveys and distinguish them in separate data frames.

## Set environment and load packages

```{r setup, include=FALSE}

knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  echo = TRUE
)
```

```{r}

library(dplyr)
library(readr)
library(tibble)
library(lubridate)
library(arrow)
library(forcats)
```

## Load data

```{r}

# This is practical to have stored so I don't have to copy it from the NTNU NICE area all the time

file_dir_NICE <- "//fil.nice.ntnu.no/nice/p758/data/my_data"

full_data <- read_parquet("//fil.nice.ntnu.no/nice/p758/data/my_data/full_data.parquet")
```

## Process data

### Basic processing

Un-capitazing names, change variable types and create date-time objects.

```{r}

# Un-capitalize and clean names

full_cleaned_data <- full_data |> 
  janitor::clean_names()

# Remove initial data frame

rm(full_data)

# Change from characterto factors

full_cleaned_data <- full_cleaned_data |>
  mutate_if(is.character, as.factor)

# Manually change actual chr back to fct again

full_cleaned_data <- full_cleaned_data |> 
  mutate(
    birth_date = as.character(birth_date), 
    end_date_death = as.character(end_date_death),
    end_date_icd8910 = as.character(end_date_icd8910),
    end_date_link = as.character(end_date_link),
    w22_0389_lopenr_person = as.character(w22_0389_lopenr_person),
    death_all = as.factor(death_all),
    death_all_icd = as.factor(death_all_icd),
    death_cvd = as.factor(death_cvd),
    death_ihd = as.factor(death_ihd),
    death_stroke = as.factor(death_stroke),
    death_ihd = as.factor(death_ihd)
    )

# Create date objects

full_cleaned_data <- full_cleaned_data |> 
  mutate(
    part_dat_nt1blq1 = dmy(part_dat_nt1blq1),
    part_dat_nt2blq1 = dmy(part_dat_nt2blq1),
    part_dat_nt3blq1 = dmy(part_dat_nt3blq1),
    part_dat_nt4blq1 = dmy(part_dat_nt4blq1),
    end_date_death = dmy(end_date_death),
    end_date_link = dmy(end_date_link),
    end_date_icd8910 = dmy(end_date_icd8910),
    eof_date_death = dmy(eof_date_death)
  )

# Compute participant age

full_cleaned_data <- full_cleaned_data |> 
  mutate(age = year(end_date_death) - birth_year)

# Write to NTNU NICE area

write_parquet(
  full_cleaned_data,
  "//fil.nice.ntnu.no/nice/p758/data/my_data/full_cleaned_data.parquet"
)
```

### Distinguish different HUNT surveys into separate data frames

#### Data frames for no change in LTPA

Since all the data for all four surveys are in one data set, we have to distinguish them into four separate data sets.

Each survey has two variables called BLM and a variable related to each sub-survey. We use these to distinguish.

#### HUNT 1

Q2 is the relevant Q for H1, it contains data on exercise, work, education, smoking and alcohol

```{r}

hunt_1_cleaned_full_data <- full_cleaned_data |> 
  select(contains(match = "nt1"), age, sex, end_date_death, death_all) |> 
  filter(!is.na(part_nt1blq2) & !is.na(part_nt1blm))

write_parquet(
  hunt_1_cleaned_full_data,
  "//fil.nice.ntnu.no/nice/p758/data/my_data/hunt_1_cleaned_full_data.parquet"
)
```

#### HUNT 2

Q1 is the relevant for Q for H2, it contains data on exercise, work, education, smoking and alcohol

```{r}

hunt_2_cleaned_full_data <- full_cleaned_data |> 
  select(contains(match = "nt2"), age, sex, end_date_death, death_all) |> 
  filter(!is.na(part_nt2blq1) & !is.na(part_nt2blm))

write_parquet(
  hunt_2_cleaned_full_data,
  "//fil.nice.ntnu.no/nice/p758/data/my_data/hunt_2_cleaned_full_data.parquet"
)
```

#### HUNT 3

Q1 is the relevant for Q for H3, it contains data on exercise, work, education, smoking and alcohol

```{r}

hunt_3_cleaned_full_data <- full_cleaned_data |> 
  select(contains(match = "nt3"), age, sex, end_date_death, death_all) |> 
  filter(!is.na(part_nt3blq1) & !is.na(part_nt3blm))

write_parquet(
  hunt_3_cleaned_full_data,
  "//fil.nice.ntnu.no/nice/p758/data/my_data/hunt_3_cleaned_full_data.parquet"
)
```

#### HUNT 4

Have no information on the surveys, so just did a filtering on baseline measurements (i.e, blm variable)

```{r}

hunt_4_cleaned_full_data <- full_cleaned_data |>
  select(contains(match = "nt4"), age, sex, end_date_death, death_all) |> 
  filter(!is.na(part_nt4blm))

write_parquet(
  hunt_4_cleaned_full_data,
  "//fil.nice.ntnu.no/nice/p758/data/my_data/hunt_4_cleaned_full_data.parquet"
)
```

#### Data frames for change in LTPA

The data processing for these data frames is done in both Stata and in R. The basic processing of distinguishing into different data frames and string manipulation is done in R, while dividing participants into LTPA trajectory groups are done in Stata and in R. Ultimately, this means that the data is written to a .csv here before Stata processing, and read in as a .csv after its been processed in Stata.

#### HUNT 1-3

Distinguish into a data frame and string manipulation:

```{r}

hunt_1_3_cleaned_full_data <- full_cleaned_data |> 
  select(contains(match = c("nt1", "nt3")), end_date_death, death_all, age, sex) |> 
  filter(
    !is.na(part_nt1blq2) & !is.na(part_nt1blm) & !is.na(part_nt3blq1) & !is.na(part_nt3blq1)
    ) |> 
   mutate(
    exe_du_nt1blq2 = case_when(
      exe_du_nt1blq2 == "16-30 minutter" ~ "15-29 minutter",
      exe_du_nt1blq2 == "30 minutter-1 time" ~ "30-60 minutter",
      exe_du_nt1blq2 == "Mer enn 1 time" ~ "Mer enn 60 minutter",
      .default = exe_du_nt1blq2
    ),
    exe_du_nt3blq1 = case_when(
      exe_du_nt3blq1 == "30 minutter - 1 time" ~ "30-60 minutter",
      exe_du_nt3blq1 == "Mer enn 1 time" ~ "Mer enn 60 minutter",
      .default = exe_du_nt3blq1
    )
   )
```

Write to disk for further Stata manipulation:

```{r}

# write_csv(
#   hunt_1_3_cleaned_full_data,
#   "//fil.nice.ntnu.no/nice/p758/Data master/hunt_1_3_cleaned_full_data.csv"
# )
```

The process is repeated for H1-H3-H4 and H3-H4 as well. **Note: These .csv files must not be written to disk in this script because they will overwrite the ones processed in Stata.**

#### HUNT 1-3-4

```{r}

hunt_1_3_4_cleaned_full_data <- full_cleaned_data |> 
  select(contains(match = c("nt1", "nt3", "nt4")), end_date_death, death_all, age, sex) |> 
  filter(
    !is.na(part_nt1blq2) & !is.na(part_nt1blm) 
    & !is.na(part_nt3blq1) & !is.na(part_nt3blq1)
    & !is.na(part_nt4blm)
    ) |> 
   mutate(
    exe_du_nt1blq2 = case_when(
      exe_du_nt1blq2 == "16-30 minutter" ~ "15-29 minutter",
      exe_du_nt1blq2 == "30 minutter-1 time" ~ "30-60 minutter",
      exe_du_nt1blq2 == "Mer enn 1 time" ~ "Mer enn 60 minutter",
      .default = exe_du_nt1blq2
    ),
    exe_du_nt3blq1 = case_when(
      exe_du_nt3blq1 == "30 minutter - 1 time" ~ "30-60 minutter",
      exe_du_nt3blq1 == "Mer enn 1 time" ~ "Mer enn 60 minutter",
      .default = exe_du_nt3blq1
    )
   )
```

```{r}

# write_csv(
#   hunt_1_3_4_cleaned_full_data,
#   "//fil.nice.ntnu.no/nice/p758/Data master/hunt_1_3_4_cleaned_full_data.csv"
# )
```

#### HUNT 3-4

```{r}

hunt_3_4_cleaned_full_data <- full_cleaned_data |> 
  select(contains(match = c("nt3", "nt4")), end_date_death, death_all, age, sex) |> 
  filter(
    !is.na(part_nt3blq1) & !is.na(part_nt3blq1) & !is.na(part_nt4blm)
    ) |> 
   mutate(
    exe_du_nt3blq1 = case_when(
      exe_du_nt3blq1 == "30 minutter - 1 time" ~ "30-60 minutter",
      exe_du_nt3blq1 == "Mer enn 1 time" ~ "Mer enn 60 minutter",
      .default = exe_du_nt3blq1
    )
   )
```

```{r}

# write_csv(
#   hunt_3_4_cleaned_full_data,
#   "//fil.nice.ntnu.no/nice/p758/Data master/hunt_3_4_cleaned_full_data.csv"
# )
```
